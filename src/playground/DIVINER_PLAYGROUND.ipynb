{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c662c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\"\"\"\n",
    "This will download all the files in the FOLDER of BASE SUFFIX. If there are also folders, not really sure what is would do, lol\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Base URL of the page\n",
    "BASE_URL = \"https://pds-geosciences.wustl.edu\"\n",
    "BASE_SUFFIX = \"/lro/lro-l-dlre-4-rdr-v1/lrodlr_1002/data/2023/202305/20230501\"\n",
    "\n",
    "# Directory to save downloaded files\n",
    "DOWNLOAD_DIR = \"/media/mglos/HDD1_8TB1/DIVINER_JUPYTER\"\n",
    "\n",
    "# File suffixes to download\n",
    "FILE_SUFFIXES = {\".zip\", \".lbl\", \".xml\"}\n",
    "\n",
    "# Maximum concurrent downloads\n",
    "MAX_DOWNLOADS = 5\n",
    "\n",
    "# Ensure the download directory exists\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "async def download_file(session, url, save_path, progress):\n",
    "    \"\"\"Download a single file asynchronously.\"\"\"\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status == 200:\n",
    "                with open(save_path, \"wb\") as file:\n",
    "                    async for chunk in response.content.iter_chunked(8192):\n",
    "                        file.write(chunk)\n",
    "                progress[\"completed\"] += 1\n",
    "                print(f\"Downloaded: {save_path} ({progress['completed']}/{progress['total']})\")\n",
    "            else:\n",
    "                print(f\"Failed to download: {url} (status: {response.status})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "async def scrape_and_download():\n",
    "    \"\"\"Scrape the page, find files with the specified suffixes, and download them.\"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        response = await session.get(BASE_URL + BASE_SUFFIX)\n",
    "        if response.status == 200:\n",
    "            soup = BeautifulSoup(await response.text(), \"html.parser\")\n",
    "            links = [\n",
    "                link.get(\"href\") for link in soup.find_all(\"a\")\n",
    "                if link.get(\"href\") and any(link.get(\"href\").endswith(suffix) for suffix in FILE_SUFFIXES)\n",
    "            ]\n",
    "\n",
    "            # Progress tracking\n",
    "            progress = {\"completed\": 0, \"total\": len(links)}\n",
    "            print(f\"Found {progress['total']} files to download.\")\n",
    "\n",
    "            # Semaphore for limiting concurrent downloads\n",
    "            semaphore = asyncio.Semaphore(MAX_DOWNLOADS)\n",
    "\n",
    "            async def limited_download(href):\n",
    "                async with semaphore:\n",
    "                    full_url = f\"{BASE_URL}{href}\"\n",
    "                    save_path = os.path.join(DOWNLOAD_DIR, href.split(\"/\")[-1])\n",
    "                    await download_file(session, full_url, save_path, progress)\n",
    "\n",
    "            # Run downloads asynchronously\n",
    "            tasks = [limited_download(href) for href in links]\n",
    "            await asyncio.gather(*tasks)\n",
    "        else:\n",
    "            print(f\"Failed to access {BASE_URL + BASE_SUFFIX} (status: {response.status})\")\n",
    "\n",
    "# Run the async function\n",
    "# await scrape_and_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c28fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "DOWNLOAD_DIR = \"/media/mglos/HDD1_8TB1/DIVINER_JUPYTER\"\n",
    "files = os.listdir(DOWNLOAD_DIR)\n",
    "data_groups = defaultdict(dict)\n",
    "for file in files:\n",
    "    key = ''.join(file.split(\".\")[:-1])\n",
    "    file_format = file.split(\".\")[-1]\n",
    "    data_groups[key.lower()][file_format.lower()] = os.path.join(DOWNLOAD_DIR, file)\n",
    "\n",
    "\n",
    "#### Extract the zipfiles, necessary only once\n",
    "# # Extract the zipfiles in which the tab files are stored\n",
    "# for base_name, file_dict in tqdm(data_groups.items(), desc=\"Extracting zip files\"):\n",
    "#     filename = file_dict.get(\"zip\")\n",
    "#     if filename is None:\n",
    "#         continue\n",
    "\n",
    "#     with zipfile.ZipFile(filename, 'r') as z:\n",
    "#         for zip_file in z.filelist:\n",
    "#             with open(os.path.join(DOWNLOAD_DIR, zip_file.filename.lower()), 'wb') as f:\n",
    "#                 f.write(z.open(zip_file).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lxml import etree\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def parse_pds4_metadata(xml_path):\n",
    "#     \"\"\"Extract field metadata from the PDS4 XML file.\"\"\"\n",
    "#     tree = etree.parse(xml_path)\n",
    "#     root = tree.getroot()\n",
    "\n",
    "#     # Define namespace for XML parsing\n",
    "#     ns = {\"pds\": \"http://pds.nasa.gov/pds4/pds/v1\"}\n",
    "\n",
    "#     # Extract field metadata from the XML\n",
    "#     fields = []\n",
    "#     for field in root.xpath(\".//pds:Field_Character\", namespaces=ns):\n",
    "#         invalid_constant = field.find(\".//pds:invalid_constant\", namespaces=ns)\n",
    "#         unknown_constant = field.find(\".//pds:unknown_constant\", namespaces=ns)\n",
    "\n",
    "#         fields.append({\n",
    "#             \"name\": field.findtext(\"pds:name\", namespaces=ns).strip(),\n",
    "#             \"field_length\": int(field.findtext(\"pds:field_length\", namespaces=ns)),\n",
    "#             \"field_location\": int(field.findtext(\"pds:field_location\", namespaces=ns)),\n",
    "#             \"special_constants\": {\n",
    "#                 \"invalid\": invalid_constant.text if invalid_constant is not None else None,\n",
    "#                 \"unknown\": unknown_constant.text if unknown_constant is not None else None,\n",
    "#             },\n",
    "#         })\n",
    "#     return fields\n",
    "\n",
    "# def load_tab_file(tab_path, fields):\n",
    "#     \"\"\"Load the .tab file using extracted metadata.\"\"\"\n",
    "#     # Extract column names and calculate dynamic widths\n",
    "#     col_names = [field[\"name\"] for field in fields]\n",
    "#     col_locations = [field[\"field_location\"] for field in fields]\n",
    "\n",
    "#     # Calculate column widths based on locations\n",
    "#     col_widths = [b - a for a, b in zip(col_locations[:-1], col_locations[1:])]\n",
    "#     col_widths.append(fields[-1][\"field_length\"])  # Add the width of the last column\n",
    "\n",
    "#     special_constants = {\n",
    "#         field[\"name\"]: (field[\"special_constants\"][\"invalid\"], field[\"special_constants\"][\"unknown\"])\n",
    "#         for field in fields if field[\"special_constants\"][\"invalid\"] or field[\"special_constants\"][\"unknown\"]\n",
    "#     }\n",
    "\n",
    "#     # Load the .tab file\n",
    "#     df = pd.read_fwf(tab_path, widths=col_widths, names=col_names, skiprows=3)\n",
    "\n",
    "#     # Replace special constants with NaN\n",
    "#     for col, constants in special_constants.items():\n",
    "#         invalid, unknown = constants\n",
    "#         to_replace = []\n",
    "#         if invalid is not None:\n",
    "#             to_replace.append(float(invalid))\n",
    "#         if unknown is not None:\n",
    "#             to_replace.append(float(unknown))\n",
    "#         df[col] = df[col].replace(to_replace, np.nan)\n",
    "\n",
    "#     # Clean up column names and extra rows\n",
    "#     df.columns = df.columns.str.strip()\n",
    "#     df = df.dropna(how=\"all\")  # Remove rows with all NaN values\n",
    "\n",
    "#     return df.replace([-9999, -9998], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8eda49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_pds4_metadata(xml_path):\n",
    "    \"\"\"Extract field metadata from the PDS4 XML file.\"\"\"\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Define namespace for XML parsing\n",
    "    ns = {\"pds\": \"http://pds.nasa.gov/pds4/pds/v1\"}\n",
    "\n",
    "    # Extract field metadata from the XML\n",
    "    fields = []\n",
    "    for field in root.xpath(\".//pds:Field_Character\", namespaces=ns):\n",
    "        invalid_constant = field.find(\".//pds:invalid_constant\", namespaces=ns)\n",
    "        unknown_constant = field.find(\".//pds:unknown_constant\", namespaces=ns)\n",
    "\n",
    "        fields.append({\n",
    "            \"name\": field.findtext(\"pds:name\", namespaces=ns).strip(),\n",
    "            \"field_length\": int(field.findtext(\"pds:field_length\", namespaces=ns)),\n",
    "            \"field_location\": int(field.findtext(\"pds:field_location\", namespaces=ns)),\n",
    "            \"special_constants\": {\n",
    "                \"invalid\": invalid_constant.text if invalid_constant is not None else None,\n",
    "                \"unknown\": unknown_constant.text if unknown_constant is not None else None,\n",
    "            },\n",
    "        })\n",
    "    return fields\n",
    "\n",
    "\n",
    "def load_tab_file(tab_path, fields):\n",
    "    \"\"\"Load the .tab file using extracted metadata.\"\"\"\n",
    "    # Extract column names and calculate dynamic widths\n",
    "    col_names = [field[\"name\"] for field in fields]\n",
    "    col_locations = [field[\"field_location\"] for field in fields]\n",
    "\n",
    "    # Calculate column widths based on locations\n",
    "    col_widths = [b - a for a, b in zip(col_locations[:-1], col_locations[1:])]\n",
    "    col_widths.append(fields[-1][\"field_length\"])  # Add the width of the last column\n",
    "\n",
    "    # Extract special constants (invalid and unknown)\n",
    "    special_constants = {\n",
    "        field[\"name\"]: [\n",
    "            float(field[\"special_constants\"][\"invalid\"]) if field[\"special_constants\"][\"invalid\"] else None,\n",
    "            float(field[\"special_constants\"][\"unknown\"]) if field[\"special_constants\"][\"unknown\"] else None\n",
    "        ]\n",
    "        for field in fields if field[\"special_constants\"][\"invalid\"] or field[\"special_constants\"][\"unknown\"]\n",
    "    }\n",
    "\n",
    "    # Load the .tab file and skip incorrect header rows\n",
    "    df = pd.read_fwf(tab_path, widths=col_widths, names=col_names, skiprows=4)\n",
    "\n",
    "    # Replace special constants with NaN\n",
    "    for col, constants in special_constants.items():\n",
    "        invalid, unknown = constants\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace([invalid, unknown], np.nan)\n",
    "\n",
    "    # Clean up column names and ensure proper formatting\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Fix string columns like 'date' and 'utc' (strip quotes and clean up)\n",
    "    for col in ['date', 'utc']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip().str.replace('\"', '', regex=True)\n",
    "\n",
    "    # Drop rows with all NaN values and reset index\n",
    "    df = df.dropna(how=\"all\").reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a638d",
   "metadata": {},
   "source": [
    "Try to read, parse, plot?, interpret? data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47085d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>utc</th>\n",
       "      <th>jdate</th>\n",
       "      <th>orbit</th>\n",
       "      <th>sundist</th>\n",
       "      <th>sunlat</th>\n",
       "      <th>sunlon</th>\n",
       "      <th>sclk</th>\n",
       "      <th>sclat</th>\n",
       "      <th>sclon</th>\n",
       "      <th>scrad</th>\n",
       "      <th>scalt</th>\n",
       "      <th>el_cmd</th>\n",
       "      <th>az_cmd</th>\n",
       "      <th>af</th>\n",
       "      <th>orientlat</th>\n",
       "      <th>orientlon</th>\n",
       "      <th>c</th>\n",
       "      <th>det</th>\n",
       "      <th>vlookx</th>\n",
       "      <th>vlooky</th>\n",
       "      <th>vlookz</th>\n",
       "      <th>radiance</th>\n",
       "      <th>tb</th>\n",
       "      <th>clat</th>\n",
       "      <th>clon</th>\n",
       "      <th>cemis</th>\n",
       "      <th>csunzen</th>\n",
       "      <th>csunazi</th>\n",
       "      <th>cloctime</th>\n",
       "      <th>qca</th>\n",
       "      <th>qge</th>\n",
       "      <th>qmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-May-2023,</td>\n",
       "      <td>06:30:00.084,</td>\n",
       "      <td>2460065.770834301,</td>\n",
       "      <td>62321,</td>\n",
       "      <td>1.00899,</td>\n",
       "      <td>0.14611,</td>\n",
       "      <td>49.11551,</td>\n",
       "      <td>0704615400.03276,</td>\n",
       "      <td>-61.71028,</td>\n",
       "      <td>352.57893,</td>\n",
       "      <td>1803.84283,</td>\n",
       "      <td>66.51757,</td>\n",
       "      <td>180.000,</td>\n",
       "      <td>240.000,</td>\n",
       "      <td>110,</td>\n",
       "      <td>-5.01548,</td>\n",
       "      <td>253.41948,</td>\n",
       "      <td>1,</td>\n",
       "      <td>1,</td>\n",
       "      <td>-0.508267,</td>\n",
       "      <td>0.037453,</td>\n",
       "      <td>0.860385,</td>\n",
       "      <td>79382.7422,</td>\n",
       "      <td>-9999.0,</td>\n",
       "      <td>-61.79865,</td>\n",
       "      <td>352.44656,</td>\n",
       "      <td>2.94114,</td>\n",
       "      <td>75.08370,</td>\n",
       "      <td>24.57522,</td>\n",
       "      <td>8.22194,</td>\n",
       "      <td>008,</td>\n",
       "      <td>012,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-May-2023,</td>\n",
       "      <td>06:30:00.084,</td>\n",
       "      <td>2460065.770834301,</td>\n",
       "      <td>62321,</td>\n",
       "      <td>1.00899,</td>\n",
       "      <td>0.14611,</td>\n",
       "      <td>49.11551,</td>\n",
       "      <td>0704615400.03276,</td>\n",
       "      <td>-61.71028,</td>\n",
       "      <td>352.57893,</td>\n",
       "      <td>1803.84283,</td>\n",
       "      <td>66.51757,</td>\n",
       "      <td>180.000,</td>\n",
       "      <td>240.000,</td>\n",
       "      <td>110,</td>\n",
       "      <td>-5.01548,</td>\n",
       "      <td>253.41948,</td>\n",
       "      <td>1,</td>\n",
       "      <td>2,</td>\n",
       "      <td>-0.507357,</td>\n",
       "      <td>0.040687,</td>\n",
       "      <td>0.860775,</td>\n",
       "      <td>83466.8906,</td>\n",
       "      <td>-9999.0,</td>\n",
       "      <td>-61.79731,</td>\n",
       "      <td>352.46204,</td>\n",
       "      <td>2.80101,</td>\n",
       "      <td>75.07671,</td>\n",
       "      <td>27.45296,</td>\n",
       "      <td>8.22306,</td>\n",
       "      <td>008,</td>\n",
       "      <td>012,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-May-2023,</td>\n",
       "      <td>06:30:00.084,</td>\n",
       "      <td>2460065.770834301,</td>\n",
       "      <td>62321,</td>\n",
       "      <td>1.00899,</td>\n",
       "      <td>0.14611,</td>\n",
       "      <td>49.11551,</td>\n",
       "      <td>0704615400.03276,</td>\n",
       "      <td>-61.71028,</td>\n",
       "      <td>352.57893,</td>\n",
       "      <td>1803.84283,</td>\n",
       "      <td>66.51757,</td>\n",
       "      <td>180.000,</td>\n",
       "      <td>240.000,</td>\n",
       "      <td>110,</td>\n",
       "      <td>-5.01548,</td>\n",
       "      <td>253.41948,</td>\n",
       "      <td>1,</td>\n",
       "      <td>3,</td>\n",
       "      <td>-0.506441,</td>\n",
       "      <td>0.043921,</td>\n",
       "      <td>0.861155,</td>\n",
       "      <td>79502.1406,</td>\n",
       "      <td>-9999.0,</td>\n",
       "      <td>-61.79597,</td>\n",
       "      <td>352.47748,</td>\n",
       "      <td>2.66870,</td>\n",
       "      <td>75.06973,</td>\n",
       "      <td>30.62492,</td>\n",
       "      <td>8.22389,</td>\n",
       "      <td>008,</td>\n",
       "      <td>012,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-May-2023,</td>\n",
       "      <td>06:30:00.084,</td>\n",
       "      <td>2460065.770834301,</td>\n",
       "      <td>62321,</td>\n",
       "      <td>1.00899,</td>\n",
       "      <td>0.14611,</td>\n",
       "      <td>49.11551,</td>\n",
       "      <td>0704615400.03276,</td>\n",
       "      <td>-61.71028,</td>\n",
       "      <td>352.57893,</td>\n",
       "      <td>1803.84283,</td>\n",
       "      <td>66.51757,</td>\n",
       "      <td>180.000,</td>\n",
       "      <td>240.000,</td>\n",
       "      <td>110,</td>\n",
       "      <td>-5.01548,</td>\n",
       "      <td>253.41948,</td>\n",
       "      <td>1,</td>\n",
       "      <td>4,</td>\n",
       "      <td>-0.505525,</td>\n",
       "      <td>0.047138,</td>\n",
       "      <td>0.861524,</td>\n",
       "      <td>79474.0469,</td>\n",
       "      <td>-9999.0,</td>\n",
       "      <td>-61.79464,</td>\n",
       "      <td>352.49286,</td>\n",
       "      <td>2.54602,</td>\n",
       "      <td>75.06278,</td>\n",
       "      <td>34.09689,</td>\n",
       "      <td>8.22500,</td>\n",
       "      <td>008,</td>\n",
       "      <td>012,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-May-2023,</td>\n",
       "      <td>06:30:00.084,</td>\n",
       "      <td>2460065.770834301,</td>\n",
       "      <td>62321,</td>\n",
       "      <td>1.00899,</td>\n",
       "      <td>0.14611,</td>\n",
       "      <td>49.11551,</td>\n",
       "      <td>0704615400.03276,</td>\n",
       "      <td>-61.71028,</td>\n",
       "      <td>352.57893,</td>\n",
       "      <td>1803.84283,</td>\n",
       "      <td>66.51757,</td>\n",
       "      <td>180.000,</td>\n",
       "      <td>240.000,</td>\n",
       "      <td>110,</td>\n",
       "      <td>-5.01548,</td>\n",
       "      <td>253.41948,</td>\n",
       "      <td>1,</td>\n",
       "      <td>5,</td>\n",
       "      <td>-0.504559,</td>\n",
       "      <td>0.050506,</td>\n",
       "      <td>0.861899,</td>\n",
       "      <td>92070.6406,</td>\n",
       "      <td>-9999.0,</td>\n",
       "      <td>-61.79324,</td>\n",
       "      <td>352.50894,</td>\n",
       "      <td>2.42861,</td>\n",
       "      <td>75.05550,</td>\n",
       "      <td>38.09282,</td>\n",
       "      <td>8.22611,</td>\n",
       "      <td>008,</td>\n",
       "      <td>012,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date            utc               jdate   orbit   sundist  \\\n",
       "0  01-May-2023,  06:30:00.084,  2460065.770834301,  62321,  1.00899,   \n",
       "1  01-May-2023,  06:30:00.084,  2460065.770834301,  62321,  1.00899,   \n",
       "2  01-May-2023,  06:30:00.084,  2460065.770834301,  62321,  1.00899,   \n",
       "3  01-May-2023,  06:30:00.084,  2460065.770834301,  62321,  1.00899,   \n",
       "4  01-May-2023,  06:30:00.084,  2460065.770834301,  62321,  1.00899,   \n",
       "\n",
       "     sunlat     sunlon               sclk       sclat       sclon  \\\n",
       "0  0.14611,  49.11551,  0704615400.03276,  -61.71028,  352.57893,   \n",
       "1  0.14611,  49.11551,  0704615400.03276,  -61.71028,  352.57893,   \n",
       "2  0.14611,  49.11551,  0704615400.03276,  -61.71028,  352.57893,   \n",
       "3  0.14611,  49.11551,  0704615400.03276,  -61.71028,  352.57893,   \n",
       "4  0.14611,  49.11551,  0704615400.03276,  -61.71028,  352.57893,   \n",
       "\n",
       "         scrad      scalt    el_cmd    az_cmd    af  orientlat   orientlon  \\\n",
       "0  1803.84283,  66.51757,  180.000,  240.000,  110,  -5.01548,  253.41948,   \n",
       "1  1803.84283,  66.51757,  180.000,  240.000,  110,  -5.01548,  253.41948,   \n",
       "2  1803.84283,  66.51757,  180.000,  240.000,  110,  -5.01548,  253.41948,   \n",
       "3  1803.84283,  66.51757,  180.000,  240.000,  110,  -5.01548,  253.41948,   \n",
       "4  1803.84283,  66.51757,  180.000,  240.000,  110,  -5.01548,  253.41948,   \n",
       "\n",
       "    c det      vlookx     vlooky     vlookz     radiance        tb  \\\n",
       "0  1,  1,  -0.508267,  0.037453,  0.860385,  79382.7422,  -9999.0,   \n",
       "1  1,  2,  -0.507357,  0.040687,  0.860775,  83466.8906,  -9999.0,   \n",
       "2  1,  3,  -0.506441,  0.043921,  0.861155,  79502.1406,  -9999.0,   \n",
       "3  1,  4,  -0.505525,  0.047138,  0.861524,  79474.0469,  -9999.0,   \n",
       "4  1,  5,  -0.504559,  0.050506,  0.861899,  92070.6406,  -9999.0,   \n",
       "\n",
       "         clat        clon     cemis    csunzen    csunazi  cloctime   qca  \\\n",
       "0  -61.79865,  352.44656,  2.94114,  75.08370,  24.57522,  8.22194,  008,   \n",
       "1  -61.79731,  352.46204,  2.80101,  75.07671,  27.45296,  8.22306,  008,   \n",
       "2  -61.79597,  352.47748,  2.66870,  75.06973,  30.62492,  8.22389,  008,   \n",
       "3  -61.79464,  352.49286,  2.54602,  75.06278,  34.09689,  8.22500,  008,   \n",
       "4  -61.79324,  352.50894,  2.42861,  75.05550,  38.09282,  8.22611,  008,   \n",
       "\n",
       "    qge  qmi  \n",
       "0  012,    0  \n",
       "1  012,    0  \n",
       "2  012,    0  \n",
       "3  012,    0  \n",
       "4  012,    0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pds4_tools import read\n",
    "import pandas as pd\n",
    "\n",
    "sample = data_groups[list(data_groups.keys())[0]]\n",
    "\n",
    "# File paths\n",
    "xml_path = sample['xml']  # Update with your XML file path\n",
    "tab_path = sample['tab']  # Update with your TAB file path\n",
    "\n",
    "# Parse metadata from the XML file\n",
    "fields_metadata = parse_pds4_metadata(xml_path)\n",
    "\n",
    "# Load the .tab file into a DataFrame\n",
    "dataframe = load_tab_file(tab_path, fields_metadata)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "dataframe.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lts3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
