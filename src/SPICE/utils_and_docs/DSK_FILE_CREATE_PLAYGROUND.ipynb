{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ck: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 185/185 [00:00<00:00, 2937.79it/s]\n",
      "Processing ck: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 719/719 [00:00<00:00, 3253.00it/s]\n",
      "Processing spk: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 3120.26it/s]\n",
      "Loading static SPICE kernels from folders: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:06<00:00, 11.45it/s]\n",
      "Loading lone SPICE kernels from files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 142.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import spiceypy\n",
    "spiceypy.tkvrsn('TOOLKIT')\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spiceypy as spice\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from datetime import timedelta\n",
    "# Get file of the script and add into sys\n",
    "sys.path.insert(0, \"/home/mglos/skola/DIP/LavaTubeSniffer\")\n",
    "from src.scripts.SPICE import utils\n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "sweep_iterator = utils.SweepIterator()\n",
    "\n",
    "computation_start = sweep_iterator.min_loaded_time\n",
    "computation_end = sweep_iterator.max_loaded_time\n",
    "\n",
    "computation_timedelta = timedelta(days=1000)\n",
    "computation_now = computation_start + computation_timedelta\n",
    "\n",
    "sweep_iterator.initiate_sweep(computation_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/media/mglos/HDD_8TB1/LOLA_DSK/{}/output7_5_percent.{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.coordinates import SphericalRepresentation\n",
    "import astropy.units as u\n",
    "\n",
    "# --- Read and transform the data ---\n",
    "xyz_file = filename.format(\"xyz\", \"xyz\")\n",
    "\n",
    "df = pd.read_csv(xyz_file, sep=\" \", names=[\"x\", \"y\", \"z\"])\n",
    "x = df[\"x\"].to_numpy()\n",
    "y = df[\"y\"].to_numpy()\n",
    "z = (df[\"z\"] / 100).to_numpy().astype(np.float32)  # elevation in meters\n",
    "\n",
    "moon_radius = 1737.400  # Mean lunar radius (Km)\n",
    "\n",
    "# Map x to longitude and y to latitude\n",
    "lon = np.interp(x, (x.min(), x.max()), (-180, 180)) * u.deg\n",
    "lat = (90 - (y - y.min())/(y.max()-y.min()) * 180) * u.deg\n",
    "lat = -lat  # Invert latitude\n",
    "\n",
    "# Compute the 3D radius and convert to Cartesian\n",
    "radius = (moon_radius + z) * u.m\n",
    "spherical_coords = SphericalRepresentation(lon, lat, radius)\n",
    "cartesian_coords = spherical_coords.to_cartesian()\n",
    "\n",
    "batch_rects = np.column_stack([\n",
    "    cartesian_coords.x.value,\n",
    "    cartesian_coords.y.value,\n",
    "    cartesian_coords.z.value\n",
    "]).astype(np.float32)\n",
    "\n",
    "# --- Reshape grid and downsample based on latitude ---\n",
    "unique_x = np.unique(x)\n",
    "unique_y = np.unique(y)\n",
    "num_cols = len(unique_x)\n",
    "num_rows = len(unique_y)\n",
    "assert num_rows * num_cols == len(x), \"Data does not form a complete grid.\"\n",
    "\n",
    "# Reshape to 2D grid (rows = latitudes, cols = longitudes)\n",
    "vertices_grid = batch_rects.reshape(num_rows, num_cols, 3)\n",
    "lon_grid = lon.value.reshape(num_rows, num_cols)\n",
    "lat_grid = lat.value.reshape(num_rows, num_cols)\n",
    "\n",
    "global_vertices_list = []  # list of downsampled row vertex arrays\n",
    "row_data = []              # list of dicts: { 'indices': global indices, 'lons': sampled longitudes }\n",
    "global_index = 0\n",
    "\n",
    "for i in range(num_rows):\n",
    "    # Use the row’s first latitude to compute density\n",
    "    row_lat = lat_grid[i, 0]\n",
    "    factor = np.cos(np.deg2rad(abs(row_lat)))\n",
    "    desired_count = max(1, int(np.round(num_cols * factor)))\n",
    "    col_indices = np.linspace(0, num_cols - 1, desired_count, dtype=int)\n",
    "    \n",
    "    row_vertices = vertices_grid[i, col_indices, :]\n",
    "    row_lons = lon_grid[i, col_indices]\n",
    "    indices = np.arange(global_index, global_index + len(col_indices))\n",
    "    global_index += len(col_indices)\n",
    "    \n",
    "    global_vertices_list.append(row_vertices)\n",
    "    row_data.append({'indices': indices, 'lons': row_lons})\n",
    "\n",
    "vertices_global = np.concatenate(global_vertices_list, axis=0)\n",
    "# vertices_global: (total_points, 3)\n",
    "\n",
    "# --- Stitch adjacent rows into triangles ---\n",
    "def stitch_rows(idxA, idxB, lonA, lonB):\n",
    "    \"\"\"Merge two rows of vertices into triangles.\n",
    "       idxA, idxB: arrays of global vertex indices for the two rows.\n",
    "       lonA, lonB: corresponding longitudes.\n",
    "    \"\"\"\n",
    "    tris = []\n",
    "    i, j = 0, 0\n",
    "    # Advance through both rows until one is exhausted.\n",
    "    while i < len(idxA)-1 and j < len(idxB)-1:\n",
    "        # Compare angular gaps to decide which edge to advance.\n",
    "        diffA = abs(lonA[i+1] - lonB[j])\n",
    "        diffB = abs(lonB[j+1] - lonA[i])\n",
    "        if diffA < diffB:\n",
    "            tris.append([idxA[i], idxB[j], idxA[i+1]])\n",
    "            i += 1\n",
    "        else:\n",
    "            tris.append([idxA[i], idxB[j], idxB[j+1]])\n",
    "            j += 1\n",
    "    # Append remaining triangles if one row still has vertices.\n",
    "    while i < len(idxA)-1:\n",
    "        tris.append([idxA[i], idxB[-1], idxA[i+1]])\n",
    "        i += 1\n",
    "    while j < len(idxB)-1:\n",
    "        tris.append([idxA[-1], idxB[j], idxB[j+1]])\n",
    "        j += 1\n",
    "    return tris\n",
    "\n",
    "triangles = []  # list of triangles (each is a list of 3 global vertex indices)\n",
    "for r in range(len(row_data) - 1):\n",
    "    idxA = row_data[r]['indices']\n",
    "    idxB = row_data[r+1]['indices']\n",
    "    lonA = row_data[r]['lons']\n",
    "    lonB = row_data[r+1]['lons']\n",
    "    \n",
    "    # Handle pole-like cases: if one row has just one vertex.\n",
    "    if len(idxA) == 1:\n",
    "        for j in range(len(idxB)-1):\n",
    "            triangles.append([idxA[0], idxB[j], idxB[j+1]])\n",
    "    elif len(idxB) == 1:\n",
    "        for i in range(len(idxA)-1):\n",
    "            triangles.append([idxA[i], idxB[0], idxA[i+1]])\n",
    "    else:\n",
    "        triangles.extend(stitch_rows(idxA, idxB, lonA, lonB))\n",
    "\n",
    "# vertices_global holds the point coordinates.\n",
    "vertices = vertices_global\n",
    "triangles = [(x+1, y+1, z+1) for x, y, z in triangles]\n",
    "# triangles is a list of connectivity index triplets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename.format(\"CACHE\", \"triangles_vertices_cachse.pkl\"), \"wb\") as f:\n",
    "    pickle.dump((triangles, vertices), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# def plot_3d_planet(points, cube=False):\n",
    "#     # if cube:\n",
    "#     #     # Recenter and scale to fit within a [-1000, 1000] cube\n",
    "#     pickle.dump((triangles, vertices), f)\n",
    "#     fig = go.Figure(data=[go.Scatter3d(\n",
    "#         x=x, y=y, z=z,\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=2,\n",
    "#             color=np.linalg.norm(points, axis=1),\n",
    "#             colorscale='Viridis',\n",
    "#             opacity=0.8\n",
    "#         )\n",
    "#     )])\n",
    "    \n",
    "#     scene_conf = dict(\n",
    "#         xaxis_title='X',\n",
    "#         yaxis_title='Y',\n",
    "#         zaxis_title='Z',\n",
    "#         aspectmode='cube'\n",
    "#     )\n",
    "    \n",
    "#     if cube:\n",
    "#         scene_conf.update({\n",
    "#             'xaxis': dict(range=[-2000, 2000]),\n",
    "#             'yaxis': dict(range=[-2000, 2000]),\n",
    "#             'zaxis': dict(range=[-2000, 2000])pickle.dump((triangles, vertices), f)is=1)\n",
    "\n",
    "# # Example usage with cube option enabled:\n",
    "# plot_3d_planet(t[:10000], cube=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename.format(\"CACHE\", \"triangles_vertices_cachse.pkl\"), \"rb\") as f:\n",
    "    triangles, vertices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15203120, 30399328, 15203120, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices = np.array(vertices)\n",
    "triangles = np.array(triangles)\n",
    "len(vertices), len(triangles), np.array(triangles).max(), np.array(triangles).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Adjusted Bounds:\n",
      "  X: -1741.818603515625 to 1737.08447265625\n",
      "  Y: -1740.106201171875 to 1734.867431640625\n",
      "  Z: -1739.318115234375 to 1737.756591796875\n",
      "Estimated Fine Voxel Count (NVXTOT): 127955981.52481353\n",
      "✅ Opened DSK file: /media/mglos/HDD_8TB1/LOLA_DSK/dsk/output7_5_percent.new_gen.dsk\n",
      "Computing spatial index...\n",
      "✅ Spatial index computed.\n",
      "Writing DSK file...\n",
      "✅ DSK segment written to /media/mglos/HDD_8TB1/LOLA_DSK/dsk/output7_5_percent.new_gen.dsk\n",
      "✅ DSK file closed.\n"
     ]
    }
   ],
   "source": [
    "# #### This is used for rect model #####\n",
    "# # 🔹 Voxel Grid & Spatial Index Parameters (Adjusted to Real Values)\n",
    "# FINSCL = 0.4  # Fine voxel scale\n",
    "# CORSCL = 2  # Coarse voxel scale\n",
    "# WORKSZ = 700_000_000\n",
    "# VOXPSZ = 2_000_000_000 # Voxel-plate pointer array size\n",
    "# VOXLSZ = 400_000_000  # Voxel-plate list array size\n",
    "# SPXISZ = 1_400_000_000  # Spatial index size\n",
    "# MAKVTL = True\n",
    "\n",
    "\n",
    "FINSCL = 6.9  # Fine voxel scale\n",
    "CORSCL = 9  # Coarse voxel scale\n",
    "WORKSZ = 400_000_000\n",
    "VOXPSZ = 45_000_000 # Voxel-plate pointer array size\n",
    "VOXLSZ = 135_000_000  # Voxel-plate list array size\n",
    "SPXISZ = 1_250_000_000  # Spatial index size\n",
    "MAKVTL = True\n",
    "\n",
    "\n",
    "dsk_output = filename.format(\"dsk\", \"new_gen.dsk\")\n",
    "\n",
    "# 🔹 STEP 3: OPEN DSK FILE FOR WRITING\n",
    "if os.path.exists(dsk_output):\n",
    "    os.remove(dsk_output)  # Remove existing file to avoid conflicts\n",
    "\n",
    "mncor1, mxcor1 = np.min(vertices[:, 0]), np.max(vertices[:, 0])\n",
    "mncor2, mxcor2 = np.min(vertices[:, 1]), np.max(vertices[:, 1])\n",
    "mncor3, mxcor3 = np.min(vertices[:, 2]), np.max(vertices[:, 2])\n",
    "\n",
    "print(f\"🔹 Adjusted Bounds:\")\n",
    "print(f\"  X: {mncor1} to {mxcor1}\")\n",
    "print(f\"  Y: {mncor2} to {mxcor2}\")\n",
    "print(f\"  Z: {mncor3} to {mxcor3}\")\n",
    "\n",
    "NVXTOT = ((mxcor1 - mncor1) / FINSCL) * ((mxcor2 - mncor2) / FINSCL) * ((mxcor3 - mncor3) / FINSCL)\n",
    "print(f\"Estimated Fine Voxel Count (NVXTOT): {NVXTOT}\")\n",
    "\n",
    "handle = spice.dskopn(dsk_output, \"Generated DSK File\", 0)  # Open DSK file\n",
    "print(f\"✅ Opened DSK file: {dsk_output}\")\n",
    "\n",
    "# 🔹 STEP 4: COMPUTE SPATIAL INDEX\n",
    "print(\"Computing spatial index...\")\n",
    "spaixd, spaixi = spice.dskmi2(\n",
    "    vertices,  # Vertex count & coordinates\n",
    "    triangles,      # Plate count & connectivity\n",
    "    FINSCL, CORSCL,        # Fine & coarse voxel scales\n",
    "    WORKSZ, VOXPSZ, VOXLSZ,# Spatial index array sizes\n",
    "    MAKVTL, SPXISZ         # Vertex-plate association flag & spatial index size\n",
    ")\n",
    "print(\"✅ Spatial index computed.\")\n",
    "\n",
    "\n",
    "# 🔹 STEP 6: WRITE DSK SEGMENT\n",
    "print(\"Writing DSK file...\")\n",
    "time_start = -1e9  # Arbitrary large negative time (valid for all time)\n",
    "time_end = 1e9  # Arbitrary large positive time\n",
    "\n",
    "CORSYS = 3 # Rectangular \n",
    "#CORSYS = 4 # Planetocentric \n",
    "CORPAR = np.zeros(10) # for rectangular, unused\n",
    "#CORPAR[0] = 1737.4 # Moon radius\n",
    "#CORPAR[1] = 0.0 # Flattening coeffictient `corpar[0] * ( 1 - corpar[1] )``\n",
    "DCLASS = 2 # Triangular mesh?? I believeNVXTOT\n",
    "\n",
    "spice.dskw02(\n",
    "    handle, 301, 1001, DCLASS, \"MOON_ME\", CORSYS, CORPAR,\n",
    "    mncor1, mxcor1, mncor2, mxcor2, mncor3, mxcor3, time_start, time_end,\n",
    "    vertices, triangles, spaixd, spaixi\n",
    ")\n",
    "print(f\"✅ DSK segment written to {dsk_output}\")\n",
    "\n",
    "# 🔹 STEP 7: CLOSE DSK FILE\n",
    "spice.dskcls(handle, True)  # Enable compression & close the file\n",
    "print(\"✅ DSK file closed.\")\n",
    "\n",
    "#### This is used for rect model END #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1737.3486"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1740.4907"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(triangles).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triangles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavatubesniffer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
